{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The aim of this exercise is to train you in debugging networks using the good old print function and also tensorboard. To simulate poor training, we will train a multilayer perceptron using the CIFAR data.\n",
    "\n",
    "1. Use the CIFAR data set reader from the first homework and read the CIFAR-10 files again. \n",
    "2. Apply random noise to the image \n",
    "3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value.\n",
    "4. Convert all labels to onehot encoding\n",
    "5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "7. Also write the cost / loss at the end of each epoch to tensorboard.\n",
    "8. Train the network with learning rates of [0.1, 0.01, 0.001]. You will notice that the network will not converge well.\n",
    "9. Submit the snapshot of the histograms for the three learning rates. Describe your observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import random\n",
    "import datetime\n",
    "\n",
    "random.seed(3457)\n",
    "\n",
    "#1. Use the CIFAR data set reader from the first homework and read the CIFAR-10 files again. \n",
    "files = glob.glob(\n",
    "    \"/Users/omerorhan/Desktop/UCSC/Deep Learning/cifar-10-batches-py/data_batch*\")\n",
    "\n",
    "def unpickle(file, types):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes').get(types)\n",
    "\n",
    "data = np.concatenate([unpickle(x, b'data') for x in files])\n",
    "labels = np.concatenate([unpickle(x, b'labels') for x in files])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. Apply random noise to the image \n",
    "mean = 1   # some constant\n",
    "std = 1    # some constant (standard deviation)\n",
    "noisy_data = data + np.random.normal(mean, std, data.shape)\n",
    "\n",
    "#3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value.\n",
    "noisy_data = noisy_data.astype(float)\n",
    "noisy_data = np.divide(noisy_data, np.amax(noisy_data[0]))\n",
    "\n",
    "noisy_data = np.float32(noisy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#4. Convert all labels to onehot encoding\n",
    "n_values = np.max(labels) + 1\n",
    "print(type(labels))\n",
    "labelsencoded = np.eye(n_values)[labels]\n",
    "labelsencoded = np.float32(labelsencoded)\n",
    "\n",
    "print(labelsencoded[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(128, 3072), dtype=float32)\nTensor(\"Placeholder_1:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "learningrate = 0.001\n",
    "nepochs = 50\n",
    "batch_size = 128\n",
    "#128,3072\n",
    "nhidden1 = 512\n",
    "nhidden2 = 256 \n",
    "nhidden3 = 128 \n",
    "ninput = noisy_data[0].flatten().shape[0]\n",
    "noutput = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size,ninput])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, noutput])\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "\n",
    "weights = \\\n",
    "{\n",
    "        'h1': tf.Variable(tf.random_normal([ninput, nhidden1])),\n",
    "        'h2': tf.Variable(tf.random_normal([nhidden1, nhidden2])),\n",
    "        'h3': tf.Variable(tf.random_normal([nhidden2, nhidden3])),\n",
    "        'out': tf.Variable(tf.random_normal([nhidden3, noutput]))\n",
    "}\n",
    "\n",
    "biases = \\\n",
    "{\n",
    "    'b1': tf.Variable(tf.random_normal([nhidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([nhidden2])),\n",
    "    'b3': tf.Variable(tf.random_normal([nhidden3])),\n",
    "    'out': tf.Variable(tf.random_normal([noutput]))\n",
    "}\n",
    "\n",
    "def multiperceptron(x):\n",
    "    l1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weights['h2']), biases['b2']))\n",
    "    l3 = tf.nn.sigmoid(tf.add(tf.matmul(l2, weights['h3']), biases['b3']))\n",
    "    outl = tf.nn.sigmoid(tf.add(tf.matmul(l3, weights['out']), biases['out']))\n",
    "    return outl\n",
    "\n",
    "model = multiperceptron(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-654d8188eb49>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "# argmax tells what index is the max in the y hat\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "#6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "# Add to tensorboard summary\n",
    "tf.summary.histogram(\"weight_1\",weights['h1'])\n",
    "tf.summary.histogram(\"weight_2\",weights['h2'])\n",
    "tf.summary.histogram(\"weight_3\",weights['h3'])\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "filename=\"/Users/omerorhan/Desktop/UCSC/Deep Learning/DeepLearningUCSC/venv/homework/Assignments\" \\\n",
    "         \"/logs/run\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S_learningrate\"+str(learningrate))\n",
    "writer = tf.summary.FileWriter(filename, tf.get_default_graph())\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def getbatch(alldata, alllabels, batch_size = 16):\n",
    "    nlabels = len(alllabels)\n",
    "    number_of_batches = nlabels//batch_size # TODO: Change 100 to nlabels\n",
    "    for batch_number in range(number_of_batches):\n",
    "        rand_index = [random.randrange(0, nlabels) for i in range(batch_size)]\n",
    "        batch_x = alldata[rand_index]\n",
    "        batch_y = np.array([alllabels[idx] for idx in rand_index])\n",
    "        yield (batch_x, batch_y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.1119, Accuracy: 0.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.0097, Accuracy: 0.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 2.0463, Accuracy: 0.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 2.0241, Accuracy: 0.3359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 1.9736, Accuracy: 0.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 1.9589, Accuracy: 0.4141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 1.9250, Accuracy: 0.3828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 1.9871, Accuracy: 0.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 1.9715, Accuracy: 0.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 2.0176, Accuracy: 0.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 1.9615, Accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 1.9520, Accuracy: 0.3828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 1.9484, Accuracy: 0.4453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 1.9778, Accuracy: 0.3281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 1.9569, Accuracy: 0.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 1.9748, Accuracy: 0.3672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 1.9038, Accuracy: 0.3984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 1.9075, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Loss: 1.9218, Accuracy: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Loss: 1.9201, Accuracy: 0.3672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 1.9159, Accuracy: 0.3828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss: 1.9877, Accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Loss: 1.9769, Accuracy: 0.3281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Loss: 1.9172, Accuracy: 0.3672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Loss: 1.9356, Accuracy: 0.3984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss: 1.9273, Accuracy: 0.3359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Loss: 1.8813, Accuracy: 0.4141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Loss: 1.9430, Accuracy: 0.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Loss: 1.8968, Accuracy: 0.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Loss: 1.9876, Accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss: 1.9240, Accuracy: 0.3984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Loss: 1.8785, Accuracy: 0.4141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Loss: 1.9026, Accuracy: 0.4453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Loss: 1.9092, Accuracy: 0.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Loss: 1.9916, Accuracy: 0.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Loss: 1.9853, Accuracy: 0.3828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Loss: 1.8821, Accuracy: 0.3984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Loss: 1.8908, Accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Loss: 1.8905, Accuracy: 0.4297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Loss: 1.9232, Accuracy: 0.3906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Loss: 1.9297, Accuracy: 0.3828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Loss: 1.8717, Accuracy: 0.4297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Loss: 1.8970, Accuracy: 0.4453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Loss: 1.9343, Accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Loss: 1.9558, Accuracy: 0.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Loss: 1.8532, Accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss: 1.8883, Accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Loss: 1.9079, Accuracy: 0.4297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Loss: 1.8892, Accuracy: 0.3984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Loss: 1.9858, Accuracy: 0.3906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch_x, batch_y in getbatch(noisy_data, labelsencoded, batch_size):\n",
    "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})\n",
    "        losscalc, accuracycalc, merged_summary = \\\n",
    "                      sess.run([loss, accuracy, merged_summary_op], feed_dict={X:batch_x, Y:batch_y})\n",
    "        writer.add_summary(merged_summary, epoch) \n",
    "        \n",
    "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy(learning rate 0.1):\n",
    "In the figure below, you will notice that the accuracy is around 10%. \n",
    "Due to learning rate(0.1), accuracy is very bad.\n",
    "\n",
    "<img src=\"hm2_img/lr0.1accuracy.png\" height=\"200px\">\n",
    "\n",
    "\n",
    "### Loss(learning rate 0.1):\n",
    "In the figure below, you will notice that the loss is around 2.2 - 2.38%. \n",
    "\n",
    "<img src=\"hm2_img/lr0.1loss.png\" height=\"200px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

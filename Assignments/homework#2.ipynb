{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The aim of this exercise is to train you in debugging networks using the good old print function and also tensorboard. To simulate poor training, we will train a multilayer perceptron using the CIFAR data.\n",
    "\n",
    "1. Use the CIFAR data set reader from the first homework and read the CIFAR-10 files again. \n",
    "2. Apply random noise to the image \n",
    "3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value.\n",
    "4. Convert all labels to onehot encoding\n",
    "5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "7. Also write the cost / loss at the end of each epoch to tensorboard.\n",
    "8. Train the network with learning rates of [0.1, 0.01, 0.001]. You will notice that the network will not converge well.\n",
    "9. Submit the snapshot of the histograms for the three learning rates. Describe your observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "\n",
    "#1. Use the CIFAR data set reader from the first homework and read the CIFAR-10 files again. \n",
    "files = glob.glob(\n",
    "    \"/Users/omerorhan/Desktop/UCSC/Deep Learning/cifar-10-batches-py/data_batch*\")\n",
    "\n",
    "def unpickle(file, types):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes').get(types)\n",
    "\n",
    "data = np.concatenate([unpickle(x, b'data') for x in files])\n",
    "labels = np.concatenate([unpickle(x, b'labels') for x in files])\n",
    "print(data.shape)\n",
    "#data = data.reshape([-1, 3, 32, 32]).astype(np.uint8)\n",
    "#oneimage = data[3000].transpose(1, 2, 0)\n",
    "#print(oneimage.shape)\n",
    "#plt.imshow(oneimage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. Apply random noise to the image \n",
    "mean = 1   # some constant\n",
    "std = 1    # some constant (standard deviation)\n",
    "noisy_data = data + np.random.normal(mean, std, data.shape)\n",
    "\n",
    "#3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value.\n",
    "noisy_data = noisy_data.astype(float)\n",
    "noisy_data = np.divide(noisy_data, np.amax(noisy_data[0]))\n",
    "\n",
    "noisy_data = np.float32(noisy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Convert all labels to onehot encoding\n",
    "n_values = np.max(labels) + 1\n",
    "labelsencoded = np.eye(n_values)[labels]\n",
    "labelsencoded = np.float32(labelsencoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_30:0\", shape=(128, 3072), dtype=float32)\nTensor(\"Placeholder_31:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "learningrate = 0.001\n",
    "nepochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "nhidden1 = 512\n",
    "nhidden2 = 256 \n",
    "nhidden3 = 128 \n",
    "ninput = noisy_data[0].flatten().shape[0]\n",
    "noutput = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size,ninput])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, noutput])\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal_282:0\", shape=(3072, 512), dtype=float32)\nTensor(\"random_normal_283:0\", shape=(512, 256), dtype=float32)\nTensor(\"random_normal_284:0\", shape=(256, 128), dtype=float32)\nTensor(\"random_normal_285:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "\n",
    "weights = \\\n",
    "{\n",
    "        'h1': tf.Variable(tf.random_normal([ninput, nhidden1])),\n",
    "        'h2': tf.Variable(tf.random_normal([nhidden1, nhidden2])),\n",
    "        'h3': tf.Variable(tf.random_normal([nhidden2, nhidden3])),\n",
    "        'out': tf.Variable(tf.random_normal([nhidden3, noutput]))\n",
    "}\n",
    "\n",
    "biases = \\\n",
    "{\n",
    "    'b1': tf.Variable(tf.random_normal([nhidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([nhidden2])),\n",
    "    'b3': tf.Variable(tf.random_normal([nhidden3])),\n",
    "    'out': tf.Variable(tf.random_normal([noutput]))\n",
    "}\n",
    "\n",
    "\n",
    "print(tf.random_normal([ninput, nhidden1]))\n",
    "print(tf.random_normal([nhidden1, nhidden2]))\n",
    "print(tf.random_normal([nhidden2, nhidden3]))\n",
    "print(tf.random_normal([nhidden3, noutput]))\n",
    "\n",
    "def multiperceptron(x):\n",
    "    l1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weights['h2']), biases['b2']))\n",
    "    l3 = tf.nn.sigmoid(tf.add(tf.matmul(l2, weights['h3']), biases['b3']))\n",
    "    outl = tf.nn.sigmoid(tf.add(tf.matmul(l3, weights['out']), biases['out']))\n",
    "    return outl\n",
    "\n",
    "model = multiperceptron(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "# argmax tells what index is the max in the y hat\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def getbatch(alldata, alllabels, batch_size = 16):\n",
    "    nlabels = len(alllabels)\n",
    "    number_of_batches = nlabels//batch_size # TODO: Change 100 to nlabels\n",
    "    for batch_number in range(number_of_batches):\n",
    "        rand_index = [random.randrange(0, nlabels) for i in range(batch_size)]\n",
    "        batch_x = alldata[rand_index]\n",
    "        batch_y = np.array([alllabels[idx] for idx in rand_index])\n",
    "        yield (batch_x, batch_y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./logs\", graph=tf.get_default_graph())\n",
    "tf.summary.scalar(\"Cost\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-cd5747f1d950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m# Use training data for optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Validate after every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlosscalc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracycalc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1137\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \"\"\"\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[0;32m--> 258\u001b[0;31m                                                                  type(fetch)))\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch_x, batch_y in getbatch(noisy_data, labelsencoded, batch_size):\n",
    "            # Use training data for optimization\n",
    "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})\n",
    "        # Validate after every epoch\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:batch_x, Y:batch_y})\n",
    "        #writer.add_summary(losscalc, epoch)\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The aim of this exercise is to train you in debugging networks using the good old print function and also tensorboard. To simulate poor training, we will train a multilayer perceptron using the CIFAR data.\n",
    "\n",
    "1. Use the CIFAR data set reader from the first homework and read the CIFAR-10 files again. \n",
    "2. Apply random noise to the image \n",
    "3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value.\n",
    "4. Convert all labels to onehot encoding\n",
    "5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "7. Also write the cost / loss at the end of each epoch to tensorboard.\n",
    "8. Train the network with learning rates of [0.1, 0.01, 0.001]. You will notice that the network will not converge well.\n",
    "9. Submit the snapshot of the histograms for the three learning rates. Describe your observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import random\n",
    "import datetime\n",
    "\n",
    "random.seed(3457)\n",
    "\n",
    "#1. Use the CIFAR data set reader from the first homework and read the CIFAR-10 files again. \n",
    "files = glob.glob(\n",
    "    \"/Users/omerorhan/Desktop/UCSC/Deep Learning/cifar-10-batches-py/data_batch*\")\n",
    "\n",
    "def unpickle(file, types):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes').get(types)\n",
    "\n",
    "data = np.concatenate([unpickle(x, b'data') for x in files])\n",
    "labels = np.concatenate([unpickle(x, b'labels') for x in files])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. Apply random noise to the image \n",
    "mean = 1   # some constant\n",
    "std = 1    # some constant (standard deviation)\n",
    "noisy_data = data + np.random.normal(mean, std, data.shape)\n",
    "\n",
    "#3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value.\n",
    "noisy_data = noisy_data.astype(float)\n",
    "noisy_data = np.divide(noisy_data, np.amax(noisy_data[0]))\n",
    "\n",
    "noisy_data = np.float32(noisy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#4. Convert all labels to onehot encoding\n",
    "n_values = np.max(labels) + 1\n",
    "print(type(labels))\n",
    "labelsencoded = np.eye(n_values)[labels]\n",
    "labelsencoded = np.float32(labelsencoded)\n",
    "\n",
    "print(labelsencoded[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_6:0\", shape=(128, 3072), dtype=float32)\nTensor(\"Placeholder_7:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "learningrate = 0.1\n",
    "nepochs = 10\n",
    "batch_size = 128\n",
    "#128,3072\n",
    "nhidden1 = 512\n",
    "nhidden2 = 256 \n",
    "nhidden3 = 128 \n",
    "ninput = noisy_data[0].flatten().shape[0]\n",
    "noutput = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size,ninput])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, noutput])\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal_32:0\", shape=(3072, 512), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal_33:0\", shape=(512, 256), dtype=float32)\nTensor(\"random_normal_34:0\", shape=(256, 128), dtype=float32)\nTensor(\"random_normal_35:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "\n",
    "weights = \\\n",
    "{\n",
    "        'h1': tf.Variable(tf.random_normal([ninput, nhidden1])),\n",
    "        'h2': tf.Variable(tf.random_normal([nhidden1, nhidden2])),\n",
    "        'h3': tf.Variable(tf.random_normal([nhidden2, nhidden3])),\n",
    "        'out': tf.Variable(tf.random_normal([nhidden3, noutput]))\n",
    "}\n",
    "\n",
    "biases = \\\n",
    "{\n",
    "    'b1': tf.Variable(tf.random_normal([nhidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([nhidden2])),\n",
    "    'b3': tf.Variable(tf.random_normal([nhidden3])),\n",
    "    'out': tf.Variable(tf.random_normal([noutput]))\n",
    "}\n",
    "\n",
    "def multiperceptron(x):\n",
    "    l1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weights['h2']), biases['b2']))\n",
    "    l3 = tf.nn.sigmoid(tf.add(tf.matmul(l2, weights['h3']), biases['b3']))\n",
    "    outl = tf.nn.sigmoid(tf.add(tf.matmul(l3, weights['out']), biases['out']))\n",
    "    return outl\n",
    "\n",
    "model = multiperceptron(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "# argmax tells what index is the max in the y hat\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "# Add to tensorboard summary\n",
    "tf.summary.histogram(\"weight_1\",weights['h1'])\n",
    "tf.summary.histogram(\"weight_2\",weights['h2'])\n",
    "tf.summary.histogram(\"weight_3\",weights['h3'])\n",
    "tf.summary.scalar(\"loss\", tf.constant(12))\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "filename=\"/Users/omerorhan/Desktop/UCSC/Deep Learning/DeepLearningUCSC/venv/homework/Assignments\" \\\n",
    "         \"/logs/run\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "writer = tf.summary.FileWriter(filename, tf.get_default_graph())\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def getbatch(alldata, alllabels, batch_size = 16):\n",
    "    nlabels = len(alllabels)\n",
    "    number_of_batches = nlabels//batch_size # TODO: Change 100 to nlabels\n",
    "    for batch_number in range(number_of_batches):\n",
    "        rand_index = [random.randrange(0, nlabels) for i in range(batch_size)]\n",
    "        batch_x = alldata[rand_index]\n",
    "        batch_y = np.array([alllabels[idx] for idx in rand_index])\n",
    "        yield (batch_x, batch_y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.3026, Accuracy: 0.0859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.3026, Accuracy: 0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 2.3026, Accuracy: 0.1016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 2.3026, Accuracy: 0.1172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 2.3026, Accuracy: 0.1016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 2.3026, Accuracy: 0.0547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 2.3026, Accuracy: 0.0703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 2.3026, Accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 2.3026, Accuracy: 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 2.3026, Accuracy: 0.0859\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch_x, batch_y in getbatch(noisy_data, labelsencoded, batch_size):\n",
    "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})\n",
    "        losscalc, accuracycalc, merged_summary = \\\n",
    "                      sess.run([loss, accuracy, merged_summary_op], feed_dict={X:batch_x, Y:batch_y})\n",
    "        writer.add_summary(merged_summary, epoch) \n",
    "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "learning_rate= 0.1\n",
    "Epoch: 0, Loss: 2.3026, Accuracy: 0.0859\n",
    "Epoch: 1, Loss: 2.3026, Accuracy: 0.0469\n",
    "Epoch: 2, Loss: 2.3026, Accuracy: 0.1016\n",
    "Epoch: 3, Loss: 2.3026, Accuracy: 0.1172\n",
    "Epoch: 4, Loss: 2.3026, Accuracy: 0.1016\n",
    "Epoch: 5, Loss: 2.3026, Accuracy: 0.0547\n",
    "Epoch: 6, Loss: 2.3026, Accuracy: 0.0703\n",
    "Epoch: 7, Loss: 2.3026, Accuracy: 0.1250\n",
    "Epoch: 8, Loss: 2.3026, Accuracy: 0.0625\n",
    "Epoch: 9, Loss: 2.3026, Accuracy: 0.0859\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

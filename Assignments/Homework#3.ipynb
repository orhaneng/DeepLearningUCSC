{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"optimizer_graph.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "We are reading MNIST data from http://yann.lecun.com/exdb/mnist/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "model_path = \"/Users/omerorhan/Desktop/UCSC/Deep Learning\" \\\n",
    "             \"/DeepLearningUCSC/venv/homework/Assignments/model.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n10000\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.num_examples) # Number of training data\n",
    "print(mnist.test.num_examples) # Number of test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of batches = 10\n"
     ]
    }
   ],
   "source": [
    "# architecture hyper-parameter\n",
    "noofdatapoints = mnist.train.num_examples\n",
    "\n",
    "learningrate = 0.001\n",
    "nepochs = 10\n",
    "batch_size = 512\n",
    "noofbatches = 10#noofdatapoints//batch_size\n",
    "print(\"no of batches =\", noofbatches)\n",
    "\n",
    "n_input = 784 # 28x28 image\n",
    "n_classes = 10 # 1 for each digit [0-9]\n",
    "dropout = 0.75 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining x and y as placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784) (?, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model using conv2d, Relu and Maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape input to 28x28 size\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cost, optimizer and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_25:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = conv_net(X, weights, biases, keep_prob)\n",
    "print(model)\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "train_min = tf.train.AdamOptimizer(learning_rate=learningrate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "weights_saver = tf.train.Saver(var_list=weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the session \n",
    "This will run the graph and use all the tensors that were previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 27867.2969, Accuracy: 0.2676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 10045.0215, Accuracy: 0.5508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 6153.5054, Accuracy: 0.6895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 4225.5430, Accuracy: 0.7910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 3483.7827, Accuracy: 0.8066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 2242.5103, Accuracy: 0.8555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 2615.6829, Accuracy: 0.8770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 2215.6050, Accuracy: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 1688.7600, Accuracy: 0.8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 1741.3367, Accuracy: 0.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.8991\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for _ in range(noofbatches):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            #batch_y = batch_y.astype(np.float32)\n",
    "            # Use training data for optimization\n",
    "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y, keep_prob: dropout})\n",
    "        # Validate after every epoch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], \n",
    "                                          feed_dict={X:batch_x, Y:batch_y, keep_prob: 1.0})\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
    "            \n",
    "    # When the training is complete and you are happy with the result\n",
    "\n",
    "    accuracycalc = sess.run(accuracy, \n",
    "                            feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0})\n",
    "    print(\"Testing accuracy: %0.4f\"%(accuracycalc))\n",
    "    \n",
    "    weights_saver.save(sess,model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet CNN with Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\nExtracting data/fashion/t10k-images-idx3-ubyte.gz\nExtracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "fmnist = input_data.read_data_sets('data/fashion', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of batches = 10\n(?, 784) (?, 10)\n"
     ]
    }
   ],
   "source": [
    "# architecture hyper-parameter\n",
    "noofdatapoints = fmnist.train.num_examples\n",
    "\n",
    "learningrate = 0.001\n",
    "nepochs = 10\n",
    "batch_size = 512\n",
    "noofbatches = 10#noofdatapoints//batch_size\n",
    "print(\"no of batches =\", noofbatches)\n",
    "\n",
    "n_input = 784 # 28x28 image\n",
    "n_classes = 10 # 1 for each digit [0-9]\n",
    "dropout = 0.75 \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape input to 28x28 size\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/omerorhan/Desktop/UCSC/Deep Learning/DeepLearningUCSC/venv/homework/Assignments/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_19:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = conv_net(X, weights, biases, keep_prob)\n",
    "print(model)\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "train_min = tf.train.AdamOptimizer(learning_rate=learningrate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/omerorhan/Desktop/UCSC/Deep Learning/DeepLearningUCSC/venv/homework/Assignments/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2511.9146, Accuracy: 0.8691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2894.0081, Accuracy: 0.8574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1720.3448, Accuracy: 0.9082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1621.0438, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 1400.0969, Accuracy: 0.9121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 836.0417, Accuracy: 0.9258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 1347.9968, Accuracy: 0.9219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 1177.0840, Accuracy: 0.9141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 1271.6121, Accuracy: 0.9316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 1061.7227, Accuracy: 0.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.9382\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    weights_saver.restore(sess, \"/Users/omerorhan/Desktop/UCSC/Deep Learning/DeepLearningUCSC/venv\"\n",
    "                        \"/homework/Assignments/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "        \n",
    "    for epoch in range(nepochs):\n",
    "        for _ in range(noofbatches):\n",
    "            batch_x, batch_y = fmnist.train.next_batch(batch_size)\n",
    "            #batch_y = batch_y.astype(np.float32)\n",
    "            # Use training data for optimization\n",
    "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y, keep_prob: dropout})\n",
    "        # Validate after every epoch\n",
    "\n",
    "        batch_x, batch_y = fmnist.train.next_batch(batch_size)\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], \n",
    "                                          feed_dict={X:batch_x, Y:batch_y, keep_prob: 1.0})\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
    "            \n",
    "    # When the training is complete and you are happy with the result\n",
    "    accuracycalc = sess.run(accuracy, \n",
    "                            feed_dict={X: fmnist.test.images, Y: fmnist.test.labels, keep_prob: 1.0})\n",
    "    print(\"Testing accuracy: %0.4f\"%(accuracycalc))\n",
    "    #print(sess.run(weightss))\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nteract": {
   "version": "0.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
